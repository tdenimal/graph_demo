{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c7a2c626adb2487b86261b71f7b1889baa67d9538915c0be7be6c3a53de2eab6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping 2à_minutes vers liste url\n",
    "#LIste url -> download texte\n",
    "#Texte -> NER\n",
    "#NER -> matrice de co-occurence\n",
    "#matrice de co-occurence -> graphe\n",
    "#graphe -> detection des communautés (louvain)\n",
    "#Industrailiser le code avec kedro -> pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "176\n"
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "_20m_paper = newspaper.build('https://www.20minutes.fr/politique/', memoize_articles=False,fetch_images=False)\n",
    "\n",
    "papers = []\n",
    "urls_set = set()\n",
    "for article in _20m_paper.articles:\n",
    "   # check to see if the article url is not within the urls_set\n",
    "   if article.url not in urls_set:\n",
    "     # add the unique article url to the set\n",
    "     urls_set.add(article.url)\n",
    "     # remove all links for article commentaires\n",
    "     if not str(article.url).endswith('commentaires'):\n",
    "        papers.append(article.url)\n",
    "\n",
    "print(len(papers)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "176\n"
    }
   ],
   "source": [
    "import newspaper\n",
    "import requests\n",
    "from newspaper.utils import BeautifulSoup\n",
    "\n",
    "papers = []\n",
    "urls_set = set()\n",
    "\n",
    "base_url = 'https://www.20minutes.fr/'\n",
    "\n",
    "_20m_paper = newspaper.build(base_url, \n",
    "fetch_images=False, memoize_articles=False)\n",
    "\n",
    "for article in _20m_paper.articles:\n",
    "   # check to see if the article url is not within the urls_set\n",
    "   if article.url not in urls_set:\n",
    "     # add the unique article url to the set\n",
    "     urls_set.add(article.url)\n",
    "     # remove all links for article commentaires\n",
    "     if not str(article.url).endswith('commentaires'):\n",
    "        papers.append(article.url)\n",
    "\n",
    " \n",
    "_20_urls = {'societe': base_url+'societe',\n",
    "             'politique': base_url+'politique',\n",
    "             'faits_divers': base_url+'faits_divers',\n",
    "             'monde': base_url+'monde',\n",
    "             'culture':base_url+'culture',\n",
    "             'people': base_url+'people',\n",
    "             'sports': base_url+'sports',\n",
    "             'economie': base_url+'economie',\n",
    "             'sciences': base_url+'sciences',\n",
    "             'planete': base_url+'planete',\n",
    "             'high-tech' : base_url+'high-tech',\n",
    "             }\n",
    "\n",
    "\n",
    "for category, url in _20_urls.items():\n",
    "   raw_html = requests.get(url)\n",
    "   soup = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "   for articles_tags in soup.findAll('div', {'class': 'articles'}):\n",
    "      for article_href in articles_tags.find_all('a', href=True):\n",
    "         if not str(article_href['href']).endswith('commentaires'):\n",
    "           urls_set.add(article_href['href'])\n",
    "           papers.append(article_href['href'])\n",
    "\n",
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30\n70\n120\n180\n250\n330\n420\n520\n630\n750\n880\n1020\n1170\n1330\n1500\n1680\n1870\n2070\n2280\n2500\n2730\n2970\n3220\n3480\n3750\n4030\n4320\n4620\n4930\n5250\n5580\n5920\n6270\n6630\n7000\n7380\n7770\n8170\n8580\n9000\n9430\n9870\n10320\n10780\n11250\n11730\n12220\n12720\n13230\n13750\n14280\n14820\n15370\n15930\n16500\n17080\n17670\n18270\n18880\n19500\n20130\n20770\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-8d63398ae276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mshow_more\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'//button[@type=\"button\"][contains(.,\"Voir plus\")]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arguments[0].click();\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_more\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "base_url = 'https://www.20minutes.fr/'\n",
    "\n",
    "papers = []\n",
    "urls_set = set()\n",
    "\n",
    "browser = webdriver.Chrome('/mnt/c/users/tdenimal/Downloads/chromedriver_win32/chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "\n",
    "browser.get(base_url+'politique')\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Voir plus\")]')))\n",
    "        browser.execute_script(\"arguments[0].click();\", show_more)\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    \n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    for articles_tags in soup.findAll('article'):\n",
    "        for article_href in articles_tags.find_all('a', href=True):\n",
    "            if not str(article_href['href']).endswith('commentaires'):\n",
    "                urls_set.add(article_href['href'])\n",
    "                papers.append(article_href['href'])\n",
    "\n",
    "    print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import fr_core_news_lg\n",
    "nlp = fr_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from newspaper import Article\n",
    "\n",
    "#ners = []\n",
    "for i,p in enumerate(papers[17462:]):\n",
    "    try:\n",
    "        article = Article(url=base_url+p[1:], language='fr')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "    except:\n",
    "        print(\"Retry article #\"+str(i))\n",
    "        pass\n",
    "\n",
    "    doc = nlp(article.text)\n",
    "    ner = [(X.text, X.label_) for X in doc.ents]\n",
    "#Return only unique tupe\n",
    "    ner = [tuple(i) for i in np.unique(ner, axis=0)]\n",
    "    ners += [ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "def create_co_occurences_matrix(allowed_words, documents):\n",
    "    word_to_id = dict(zip(allowed_words, range(len(allowed_words))))\n",
    "    #List of ids in each document\n",
    "    documents_as_ids = [np.sort([word_to_id[w] for w in doc if w in word_to_id]).astype('uint32') for doc in documents]\n",
    "\n",
    "    row_ind, col_ind = zip(*itertools.chain(*[[(i, w) for w in doc] for i, doc in enumerate(documents_as_ids)]))\n",
    "    data = np.ones(len(row_ind), dtype='uint32')  # use unsigned int for better memory utilization\n",
    "    max_word_id = max(itertools.chain(*documents_as_ids)) + 1\n",
    "    docs_words_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(len(documents_as_ids), max_word_id))  # efficient arithmetic operations with CSR * CSR\n",
    "    words_cooc_matrix = docs_words_matrix.T * docs_words_matrix  # multiplying docs_words_matrix with its transpose matrix would generate the co-occurences matrix\n",
    "    words_cooc_matrix.setdiag(0)\n",
    "    return words_cooc_matrix, word_to_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "repair_dict = {\n",
    "    \"('Sipa','ORG')\":\"('SIPA','ORG')\",\n",
    "    \"('du Havre', 'LOC')\":\"('Le Havre', 'LOC')\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4951"
     },
     "metadata": {},
     "execution_count": 490
    }
   ],
   "source": [
    "unique_values = [tuple(i) for i in np.unique(np.concatenate(ners), axis=0)]\n",
    "len(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Delete\n",
    "\n",
    "\n",
    "del_list = ['#RTLMatin']\n",
    "\n",
    "\n",
    "for d in ners:\n",
    "    for w in d:\n",
    "        #for a in range(0,32):\n",
    "        if 'TWITTER' in w[0]:\n",
    "            #wr = w[0].replace(\"É\",\"E\")\n",
    "            #wr = w[0].upper()\n",
    "            #print(w[0].replace(\"é\",\"e\"))\n",
    "            d.remove(w)\n",
    "            #d.append(tuple([wr,w[1]]))\n",
    "            #print(w[0].upper())\n",
    "            #d.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_cooc_matrix, word_to_id = create_co_occurences_matrix(unique_values, ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create inv id-> word\n",
    "id_to_word = {}\n",
    "for k, v in word_to_id.items():\n",
    "    id_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "\n",
    "class App:\n",
    "\n",
    "    def __init__(self, uri):\n",
    "        self.driver = GraphDatabase.driver(uri)\n",
    "\n",
    "    def close(self):\n",
    "        # Don't forget to close the driver connection when you are finished with it\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_entity(self, entity_name,entity_type):\n",
    "        with self.driver.session() as session:\n",
    "            # Write transactions allow the driver to handle retries and transient errors\n",
    "            session.write_transaction(self._create_entity,entity_name,entity_type)\n",
    "\n",
    "    def create_relationship(self, entity1_name,\n",
    "                            entity1_type,\n",
    "                            entity2_name,\n",
    "                            entity2_type,\n",
    "                            weight):\n",
    "        with self.driver.session() as session:\n",
    "            # Write transactions allow the driver to handle retries and transient errors\n",
    "            session.write_transaction(self._create_relationship,entity1_name,entity1_type,\n",
    "                                     entity2_name,entity2_type,weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_entity(tx, entity_name,entity_type):\n",
    "\n",
    "        if entity_type == \"PER\":\n",
    "            query = (\n",
    "                \"CREATE (p1:Person { name: $entity_name }) \"\n",
    "                \"RETURN p1\"\n",
    "            )\n",
    "        elif entity_type == \"ORG\":\n",
    "            query = (\n",
    "                \"CREATE (p1:Org { name: $entity_name }) \"\n",
    "                \"RETURN p1\"\n",
    "            )\n",
    "        elif entity_type == \"LOC\":\n",
    "            query = (\n",
    "                \"CREATE (p1:Location { name: $entity_name }) \"\n",
    "                \"RETURN p1\"\n",
    "            )\n",
    "        else:\n",
    "            query = (\n",
    "                \"CREATE (p1:Misc { name: $entity_name }) \"\n",
    "                \"RETURN p1\"\n",
    "            )\n",
    "\n",
    "        result = tx.run(query, entity_name=entity_name,entity_type=entity_type)\n",
    "        try:\n",
    "            return [{\"p1\": record[\"p1\"][\"name\"]}\n",
    "                    for record in result]\n",
    "        # Capture any errors along with the query and data for traceability\n",
    "        except ServiceUnavailable as exception:\n",
    "            logging.error(\"{query} raised an error: \\n {exception}\".format(\n",
    "                query=query, exception=exception))\n",
    "            raise\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_relationship(tx, entity1_name,entity1_type, entity2_name,entity2_type,weight):\n",
    "\n",
    "        # To learn more about the Cypher syntax,\n",
    "        # see https://neo4j.com/docs/cypher-manual/current/\n",
    "\n",
    "        # The Reference Card is also a good resource for keywords,\n",
    "        # see https://neo4j.com/docs/cypher-refcard/current/\n",
    "\n",
    "        if entity1_type == \"PER\":\n",
    "            ent1 = \"MATCH (p1: Person { name: $entity1_name })\"\n",
    "        elif entity1_type == \"ORG\":\n",
    "            ent1 = \"MATCH (p1: Org { name: $entity1_name })\"\n",
    "        elif entity1_type == \"LOC\":\n",
    "            ent1 = \"MATCH (p1: Location { name: $entity1_name })\"\n",
    "        else:\n",
    "            ent1 = \"MATCH (p1: Misc { name: $entity1_name })\"\n",
    "\n",
    "        if entity2_type == \"PER\":\n",
    "            ent2 = \"MATCH (p2: Person { name: $entity2_name })\"\n",
    "        elif entity2_type == \"ORG\":\n",
    "            ent2 = \"MATCH (p2: Org { name: $entity2_name })\"\n",
    "        elif entity2_type == \"LOC\":\n",
    "            ent2 = \"MATCH (p2: Location { name: $entity2_name })\"\n",
    "        else:\n",
    "            ent2 = \"MATCH (p2: Misc { name: $entity2_name })\"\n",
    "\n",
    "        query = (\n",
    "            ent1 + ent2 +\n",
    "            \"CREATE (p1)-[:co{ weight: $weight }]->(p2) \"\n",
    "            \"RETURN p1, p2\"\n",
    "        )\n",
    "        result = tx.run(query, \n",
    "                        entity1_name=entity1_name, \n",
    "                        entity2_name=entity2_name,\n",
    "                        entity1_type=entity1_type, \n",
    "                        entity2_type=entity2_type,\n",
    "                        weight=weight)\n",
    "        try:\n",
    "            return [{\"p1\": record[\"p1\"][\"name\"], \"p2\": record[\"p2\"][\"name\"]}\n",
    "                    for record in result]\n",
    "        # Capture any errors along with the query and data for traceability\n",
    "        except ServiceUnavailable as exception:\n",
    "            logging.error(\"{query} raised an error: \\n {exception}\".format(\n",
    "                query=query, exception=exception))\n",
    "            raise\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # See https://neo4j.com/developer/aura-connect-driver/ for Aura specific connection URL.\n",
    "#     scheme = \"bolt\"  # Connecting to Aura, use the \"neo4j+s\" URI scheme\n",
    "#     host_name = \"localhost\"\n",
    "#     port = 7687\n",
    "#     url = \"{scheme}://{host_name}:{port}\".format(scheme=scheme, host_name=host_name, port=port)\n",
    "#     app = App(url)\n",
    "#     app.create_friendship(\"Alice\", \"David\")\n",
    "#     app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create all entities\n",
    "\n",
    "scheme = \"neo4j\"  # Connecting to Aura, use the \"neo4j+s\" URI scheme\n",
    "#     \n",
    "host_name = \"localhost\"\n",
    "port = 11003\n",
    "\n",
    "url = \"{scheme}://{host_name}:{port}\".format(scheme=scheme, host_name=host_name, port=port)\n",
    "app = App(url)\n",
    "\n",
    "#Create entities\n",
    "for k in word_to_id.keys():\n",
    "    app.create_entity(k[0],k[1])\n",
    "\n",
    "#Create relationships\n",
    "for l in range(0,words_cooc_matrix.shape[0]):\n",
    "    for c in range(0, words_cooc_matrix.shape[1]):\n",
    "        if words_cooc_matrix[l,c] > 0:\n",
    "            app.create_relationship(id_to_word[l][0],id_to_word[l][1],\n",
    "                                    id_to_word[c][0],id_to_word[c][1],int(words_cooc_matrix[l,c]))\n",
    "\n",
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}